{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "03-part-of-speech-tagging-colab",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Zz6eeZkR0dA"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Uls0XS_R5Ie",
        "outputId": "0fe106a5-9545-4f9b-e0ad-7410dd78e8af"
      },
      "source": [
        "sentence = 'I like you'\n",
        "token = nltk.word_tokenize(sentence)\n",
        "token"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['I', 'like', 'you']"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MRAe6nohTDn7",
        "outputId": "b4514243-6c8d-424b-f533-bcdc85c63068"
      },
      "source": [
        "nltk.pos_tag(token)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('I', 'PRP'), ('like', 'VBP'), ('you', 'PRP')]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_IEoBrFeTI4V",
        "outputId": "2fa00939-0169-40ea-99cf-c1d4b2ae9ae1"
      },
      "source": [
        "sentence = 'I am like you'\n",
        "token = nltk.word_tokenize(sentence)\n",
        "nltk.pos_tag(token)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('I', 'PRP'), ('am', 'VBP'), ('like', 'IN'), ('you', 'PRP')]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Cemm8ZlTlFS"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wTQ8YR151eNK"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xA7vk-3B1eQm"
      },
      "source": [
        "!pip install pythainlp[full]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ajwedoiy1eUm"
      },
      "source": [
        "import pythainlp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7NQtu7wK1e7Z"
      },
      "source": [
        "from pythainlp import sent_tokenize, word_tokenize"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J6-no87m15Vj"
      },
      "source": [
        "text = 'หัวหน้าฮงเป็นตัวอย่างของคนที่ดีเกินไปจนทำร้ายตัวเอง'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wTb_xcIp2C7-",
        "outputId": "fcde74fd-0fc6-49bf-b3ad-91867778f955"
      },
      "source": [
        "sent = word_tokenize(text, engine='newmm')\n",
        "print(sent)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['หัวหน้า', 'ฮง', 'เป็น', 'ตัวอย่าง', 'ของ', 'คน', 'ที่', 'ดี', 'เกินไป', 'จน', 'ทำร้าย', 'ตัวเอง']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fj6AaQbt2EXb",
        "outputId": "bb7ca9b7-0afa-447a-f0c9-c83f18be481a"
      },
      "source": [
        "from pythainlp import Tokenizer\n",
        "from pythainlp.util import Trie\n",
        "from pythainlp.corpus.common import thai_words\n",
        "words = [\"ดีเกินไป\"]\n",
        "custom_words_list = set(thai_words())\n",
        "custom_words_list.update(words)\n",
        "trie = Trie(words=custom_words_list)\n",
        "custom_tokenizer = Tokenizer(custom_dict=trie, engine='newmm')\n",
        "sent = custom_tokenizer.word_tokenize(text)\n",
        "print(sent)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['หัวหน้า', 'ฮง', 'เป็น', 'ตัวอย่าง', 'ของ', 'คน', 'ที่', 'ดีเกินไป', 'จน', 'ทำร้าย', 'ตัวเอง']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aMnit0ft4JNO"
      },
      "source": [
        "from pythainlp.tag import pos_tag, pos_tag_sents"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dt7qErdn6HYp",
        "outputId": "b04a5cc5-330f-45c6-bdc5-6f463e459e42"
      },
      "source": [
        "pos_tag(sent) # Default engine=\"perceptron\" # (default) corpus=\"orchid\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('หัวหน้า', 'NCMN'),\n",
              " ('ฮง', 'NCMN'),\n",
              " ('เป็น', 'VSTA'),\n",
              " ('ตัวอย่าง', 'NCMN'),\n",
              " ('ของ', 'RPRE'),\n",
              " ('คน', 'NCMN'),\n",
              " ('ที่', 'PREL'),\n",
              " ('ดีเกินไป', 'VSTA'),\n",
              " ('จน', 'JSBR'),\n",
              " ('ทำร้าย', 'DONM'),\n",
              " ('ตัวเอง', 'PDMN')]"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lEcqRutX6KxY",
        "outputId": "81def075-4460-4fb6-80eb-39a3735b8a21"
      },
      "source": [
        "pos_tag(sent, engine=\"unigram\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('หัวหน้า', 'NCMN'),\n",
              " ('ฮง', ''),\n",
              " ('เป็น', 'VSTA'),\n",
              " ('ตัวอย่าง', 'NCMN'),\n",
              " ('ของ', 'RPRE'),\n",
              " ('คน', 'CNIT'),\n",
              " ('ที่', 'PREL'),\n",
              " ('ดีเกินไป', ''),\n",
              " ('จน', 'JSBR'),\n",
              " ('ทำร้าย', ''),\n",
              " ('ตัวเอง', 'PDMN')]"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JwhfSIcl6SX5",
        "outputId": "db43c192-4952-4869-8f41-425013088588"
      },
      "source": [
        "pos_tag(sent, corpus=\"orchid_ud\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('หัวหน้า', 'NOUN'),\n",
              " ('ฮง', 'NOUN'),\n",
              " ('เป็น', 'VERB'),\n",
              " ('ตัวอย่าง', 'NOUN'),\n",
              " ('ของ', 'ADP'),\n",
              " ('คน', 'NOUN'),\n",
              " ('ที่', 'SCONJ'),\n",
              " ('ดีเกินไป', 'VERB'),\n",
              " ('จน', 'SCONJ'),\n",
              " ('ทำร้าย', 'ADJ'),\n",
              " ('ตัวเอง', 'PRON')]"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R8DUWt2m6l8N",
        "outputId": "63ef08e9-35bf-4b21-bf29-3e52c5ebd00e"
      },
      "source": [
        "pos_tag(sent, corpus=\"pud\") #pud - Parallel Universal Dependencies (PUD)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('หัวหน้า', 'NOUN'),\n",
              " ('ฮง', 'PROPN'),\n",
              " ('เป็น', 'AUX'),\n",
              " ('ตัวอย่าง', 'NOUN'),\n",
              " ('ของ', 'ADP'),\n",
              " ('คน', 'NOUN'),\n",
              " ('ที่', 'DET'),\n",
              " ('ดีเกินไป', 'VERB'),\n",
              " ('จน', 'ADP'),\n",
              " ('ทำร้าย', 'ADP'),\n",
              " ('ตัวเอง', 'PRON')]"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_P8am42P6rOD"
      },
      "source": [
        "ิิ"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GKRw6n5g-DT7"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J9Nlk6Pu-DXK"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GeeysfRr-DaW",
        "outputId": "0064d49a-0080-4872-eb38-40551e8a5c43"
      },
      "source": [
        "import requests\n",
        " \n",
        "url = \"https://api.aiforthai.in.th/tpos\"\n",
        "text = 'หัวหน้าฮงเป็นตัวอย่างของคนที่ดีเกินไปจนทำร้ายตัวเอง'\n",
        "\n",
        "params = {'text':text}\n",
        " \n",
        "headers = {\n",
        "    'Apikey': \"\",\n",
        "    }\n",
        " \n",
        "response = requests.get(url, headers=headers, params=params)\n",
        " \n",
        "res = response.json()\n",
        "\n",
        "for word, pos in zip(res['words'], res['tags']):\n",
        "  print(word, pos)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "หัวหน้า NN\n",
            "ฮง NR\n",
            "เป็น VV\n",
            "ตัวอย่าง NN\n",
            "ของ P\n",
            "คน NN\n",
            "ที่ COMP\n",
            "ดี VA\n",
            "เกิน ADV\n",
            "ไป VV\n",
            "จน CNJ\n",
            "ทำร้าย VV\n",
            "ตัว PPER\n",
            "เอง PPER\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jq7F4lCV-acP"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yAHMWBOoAchk"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GtNSFWLoAck1"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jQEejsMDBW7X"
      },
      "source": [
        "Ref https://github.com/AiswaryaSrinivas/DataScienceWithPython"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aqnX3FQNAq66"
      },
      "source": [
        "!pip install sklearn_crfsuite"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EOc9CWQpAcon"
      },
      "source": [
        "import nltk, re, pprint\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import requests\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pprint, time\n",
        "import random\n",
        "from sklearn.model_selection import train_test_split\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn_crfsuite import CRF\n",
        "from sklearn_crfsuite import metrics\n",
        "from sklearn_crfsuite import scorers\n",
        "from collections import Counter"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fn-mA_uZBC5K"
      },
      "source": [
        "PennTree Bank Corpus, with the universal Tag Set. This tag set h"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "riWFvGVSBHap"
      },
      "source": [
        "import nltk\n",
        "nltk.download('treebank')\n",
        "nltk.download('universal_tagset')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hwOxlHvtAo6X"
      },
      "source": [
        "tagged_sentence = nltk.corpus.treebank.tagged_sents(tagset='universal')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "14RHc43HA-CB",
        "outputId": "1e61e7b7-2624-4fbd-b206-fee51d84a262"
      },
      "source": [
        "print(\"Number of Tagged Sentences \",len(tagged_sentence))\n",
        "tagged_words=[tup for sent in tagged_sentence for tup in sent]\n",
        "print(\"Total Number of Tagged words\", len(tagged_words))\n",
        "vocab=set([word for word,tag in tagged_words])\n",
        "print(\"Vocabulary of the Corpus\",len(vocab))\n",
        "tags=set([tag for word,tag in tagged_words])\n",
        "print(\"Number of Tags in the Corpus \",len(tags))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Tagged Sentences  3914\n",
            "Total Number of Tagged words 100676\n",
            "Vocabulary of the Corpus 12408\n",
            "Number of Tags in the Corpus  12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W5OUOQ52BNgg",
        "outputId": "7f3941cf-ee18-44e5-b3c1-fb7aae87776c"
      },
      "source": [
        "train_set, test_set = train_test_split(tagged_sentence,test_size=0.2,random_state=1234)\n",
        "print(\"Number of Sentences in Training Data \",len(train_set))\n",
        "print(\"Number of Sentences in Testing Data \",len(test_set))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Sentences in Training Data  3131\n",
            "Number of Sentences in Testing Data  783\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7pcjTyDTBhNk"
      },
      "source": [
        "Define the feature function. The following features can be used\n",
        "Is the first letter capitalised.\n",
        "\n",
        "1.  Is the first letter capitalised.\n",
        "2.  Is it the first word in the sentence?\n",
        "3.  Is it the last word?\n",
        "4.  What is the prefix of the word?\n",
        "5.  What is the suffix of the word?\n",
        "6.  Is the complete word captilised?\n",
        "7.  What is the previous word?\n",
        "8.  What is the next word?\n",
        "9.   Is it numeric?\n",
        "10.  Is it alphanumeric?\n",
        "11.  Is there an hyphen in the word?\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yj_OstbIBeC8"
      },
      "source": [
        "def features(sentence,index):\n",
        "    ### sentence is of the form [w1,w2,w3,..], index is the position of the word in the sentence\n",
        "    return {\n",
        "        'is_first_capital':int(sentence[index][0].isupper()),\n",
        "        'is_first_word': int(index==0),\n",
        "        'is_last_word':int(index==len(sentence)-1),\n",
        "        'is_complete_capital': int(sentence[index].upper()==sentence[index]),\n",
        "        'prev_word':'' if index==0 else sentence[index-1],\n",
        "        'next_word':'' if index==len(sentence)-1 else sentence[index+1],\n",
        "        'is_numeric':int(sentence[index].isdigit()),\n",
        "        'is_alphanumeric': int(bool((re.match('^(?=.*[0-9]$)(?=.*[a-zA-Z])',sentence[index])))),\n",
        "        'prefix_1':sentence[index][0],\n",
        "        'prefix_2': sentence[index][:2],\n",
        "        'prefix_3':sentence[index][:3],\n",
        "        'prefix_4':sentence[index][:4],\n",
        "        'suffix_1':sentence[index][-1],\n",
        "        'suffix_2':sentence[index][-2:],\n",
        "        'suffix_3':sentence[index][-3:],\n",
        "        'suffix_4':sentence[index][-4:],\n",
        "        'word_has_hyphen': 1 if '-' in sentence[index] else 0\n",
        "        \n",
        "        \n",
        "    }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NCEHhkNkCQhW"
      },
      "source": [
        "Need to seperate labels and the sentences in both training and test data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QtQshzyyCLnU"
      },
      "source": [
        "def untag(sentence):\n",
        "    return [word for word,tag in sentence]\n",
        "\n",
        "\n",
        "def prepareData(tagged_sentences):\n",
        "    X,y=[],[]\n",
        "    for sentences in tagged_sentences:\n",
        "        X.append([features(untag(sentences), index) for index in range(len(sentences))])\n",
        "        y.append([tag for word,tag in sentences])\n",
        "    return X,y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jVEWuqKpCSfP"
      },
      "source": [
        "X_train,y_train=prepareData(train_set)\n",
        "X_test,y_test=prepareData(test_set)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7PSySWB7CU_K",
        "outputId": "f7b24077-9ad8-412d-badd-321fb3be255d"
      },
      "source": [
        "X_train[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'is_alphanumeric': 0,\n",
              "  'is_complete_capital': 0,\n",
              "  'is_first_capital': 1,\n",
              "  'is_first_word': 1,\n",
              "  'is_last_word': 0,\n",
              "  'is_numeric': 0,\n",
              "  'next_word': 'Wall',\n",
              "  'prefix_1': 'O',\n",
              "  'prefix_2': 'On',\n",
              "  'prefix_3': 'On',\n",
              "  'prefix_4': 'On',\n",
              "  'prev_word': '',\n",
              "  'suffix_1': 'n',\n",
              "  'suffix_2': 'On',\n",
              "  'suffix_3': 'On',\n",
              "  'suffix_4': 'On',\n",
              "  'word_has_hyphen': 0},\n",
              " {'is_alphanumeric': 0,\n",
              "  'is_complete_capital': 0,\n",
              "  'is_first_capital': 1,\n",
              "  'is_first_word': 0,\n",
              "  'is_last_word': 0,\n",
              "  'is_numeric': 0,\n",
              "  'next_word': 'Street',\n",
              "  'prefix_1': 'W',\n",
              "  'prefix_2': 'Wa',\n",
              "  'prefix_3': 'Wal',\n",
              "  'prefix_4': 'Wall',\n",
              "  'prev_word': 'On',\n",
              "  'suffix_1': 'l',\n",
              "  'suffix_2': 'll',\n",
              "  'suffix_3': 'all',\n",
              "  'suffix_4': 'Wall',\n",
              "  'word_has_hyphen': 0},\n",
              " {'is_alphanumeric': 0,\n",
              "  'is_complete_capital': 0,\n",
              "  'is_first_capital': 1,\n",
              "  'is_first_word': 0,\n",
              "  'is_last_word': 0,\n",
              "  'is_numeric': 0,\n",
              "  'next_word': 'men',\n",
              "  'prefix_1': 'S',\n",
              "  'prefix_2': 'St',\n",
              "  'prefix_3': 'Str',\n",
              "  'prefix_4': 'Stre',\n",
              "  'prev_word': 'Wall',\n",
              "  'suffix_1': 't',\n",
              "  'suffix_2': 'et',\n",
              "  'suffix_3': 'eet',\n",
              "  'suffix_4': 'reet',\n",
              "  'word_has_hyphen': 0},\n",
              " {'is_alphanumeric': 0,\n",
              "  'is_complete_capital': 0,\n",
              "  'is_first_capital': 0,\n",
              "  'is_first_word': 0,\n",
              "  'is_last_word': 0,\n",
              "  'is_numeric': 0,\n",
              "  'next_word': 'and',\n",
              "  'prefix_1': 'm',\n",
              "  'prefix_2': 'me',\n",
              "  'prefix_3': 'men',\n",
              "  'prefix_4': 'men',\n",
              "  'prev_word': 'Street',\n",
              "  'suffix_1': 'n',\n",
              "  'suffix_2': 'en',\n",
              "  'suffix_3': 'men',\n",
              "  'suffix_4': 'men',\n",
              "  'word_has_hyphen': 0},\n",
              " {'is_alphanumeric': 0,\n",
              "  'is_complete_capital': 0,\n",
              "  'is_first_capital': 0,\n",
              "  'is_first_word': 0,\n",
              "  'is_last_word': 0,\n",
              "  'is_numeric': 0,\n",
              "  'next_word': 'women',\n",
              "  'prefix_1': 'a',\n",
              "  'prefix_2': 'an',\n",
              "  'prefix_3': 'and',\n",
              "  'prefix_4': 'and',\n",
              "  'prev_word': 'men',\n",
              "  'suffix_1': 'd',\n",
              "  'suffix_2': 'nd',\n",
              "  'suffix_3': 'and',\n",
              "  'suffix_4': 'and',\n",
              "  'word_has_hyphen': 0},\n",
              " {'is_alphanumeric': 0,\n",
              "  'is_complete_capital': 0,\n",
              "  'is_first_capital': 0,\n",
              "  'is_first_word': 0,\n",
              "  'is_last_word': 0,\n",
              "  'is_numeric': 0,\n",
              "  'next_word': 'walk',\n",
              "  'prefix_1': 'w',\n",
              "  'prefix_2': 'wo',\n",
              "  'prefix_3': 'wom',\n",
              "  'prefix_4': 'wome',\n",
              "  'prev_word': 'and',\n",
              "  'suffix_1': 'n',\n",
              "  'suffix_2': 'en',\n",
              "  'suffix_3': 'men',\n",
              "  'suffix_4': 'omen',\n",
              "  'word_has_hyphen': 0},\n",
              " {'is_alphanumeric': 0,\n",
              "  'is_complete_capital': 0,\n",
              "  'is_first_capital': 0,\n",
              "  'is_first_word': 0,\n",
              "  'is_last_word': 0,\n",
              "  'is_numeric': 0,\n",
              "  'next_word': 'with',\n",
              "  'prefix_1': 'w',\n",
              "  'prefix_2': 'wa',\n",
              "  'prefix_3': 'wal',\n",
              "  'prefix_4': 'walk',\n",
              "  'prev_word': 'women',\n",
              "  'suffix_1': 'k',\n",
              "  'suffix_2': 'lk',\n",
              "  'suffix_3': 'alk',\n",
              "  'suffix_4': 'walk',\n",
              "  'word_has_hyphen': 0},\n",
              " {'is_alphanumeric': 0,\n",
              "  'is_complete_capital': 0,\n",
              "  'is_first_capital': 0,\n",
              "  'is_first_word': 0,\n",
              "  'is_last_word': 0,\n",
              "  'is_numeric': 0,\n",
              "  'next_word': 'great',\n",
              "  'prefix_1': 'w',\n",
              "  'prefix_2': 'wi',\n",
              "  'prefix_3': 'wit',\n",
              "  'prefix_4': 'with',\n",
              "  'prev_word': 'walk',\n",
              "  'suffix_1': 'h',\n",
              "  'suffix_2': 'th',\n",
              "  'suffix_3': 'ith',\n",
              "  'suffix_4': 'with',\n",
              "  'word_has_hyphen': 0},\n",
              " {'is_alphanumeric': 0,\n",
              "  'is_complete_capital': 0,\n",
              "  'is_first_capital': 0,\n",
              "  'is_first_word': 0,\n",
              "  'is_last_word': 0,\n",
              "  'is_numeric': 0,\n",
              "  'next_word': 'purpose',\n",
              "  'prefix_1': 'g',\n",
              "  'prefix_2': 'gr',\n",
              "  'prefix_3': 'gre',\n",
              "  'prefix_4': 'grea',\n",
              "  'prev_word': 'with',\n",
              "  'suffix_1': 't',\n",
              "  'suffix_2': 'at',\n",
              "  'suffix_3': 'eat',\n",
              "  'suffix_4': 'reat',\n",
              "  'word_has_hyphen': 0},\n",
              " {'is_alphanumeric': 0,\n",
              "  'is_complete_capital': 0,\n",
              "  'is_first_capital': 0,\n",
              "  'is_first_word': 0,\n",
              "  'is_last_word': 0,\n",
              "  'is_numeric': 0,\n",
              "  'next_word': ',',\n",
              "  'prefix_1': 'p',\n",
              "  'prefix_2': 'pu',\n",
              "  'prefix_3': 'pur',\n",
              "  'prefix_4': 'purp',\n",
              "  'prev_word': 'great',\n",
              "  'suffix_1': 'e',\n",
              "  'suffix_2': 'se',\n",
              "  'suffix_3': 'ose',\n",
              "  'suffix_4': 'pose',\n",
              "  'word_has_hyphen': 0},\n",
              " {'is_alphanumeric': 0,\n",
              "  'is_complete_capital': 1,\n",
              "  'is_first_capital': 0,\n",
              "  'is_first_word': 0,\n",
              "  'is_last_word': 0,\n",
              "  'is_numeric': 0,\n",
              "  'next_word': '*-2',\n",
              "  'prefix_1': ',',\n",
              "  'prefix_2': ',',\n",
              "  'prefix_3': ',',\n",
              "  'prefix_4': ',',\n",
              "  'prev_word': 'purpose',\n",
              "  'suffix_1': ',',\n",
              "  'suffix_2': ',',\n",
              "  'suffix_3': ',',\n",
              "  'suffix_4': ',',\n",
              "  'word_has_hyphen': 0},\n",
              " {'is_alphanumeric': 0,\n",
              "  'is_complete_capital': 1,\n",
              "  'is_first_capital': 0,\n",
              "  'is_first_word': 0,\n",
              "  'is_last_word': 0,\n",
              "  'is_numeric': 0,\n",
              "  'next_word': 'noticing',\n",
              "  'prefix_1': '*',\n",
              "  'prefix_2': '*-',\n",
              "  'prefix_3': '*-2',\n",
              "  'prefix_4': '*-2',\n",
              "  'prev_word': ',',\n",
              "  'suffix_1': '2',\n",
              "  'suffix_2': '-2',\n",
              "  'suffix_3': '*-2',\n",
              "  'suffix_4': '*-2',\n",
              "  'word_has_hyphen': 1},\n",
              " {'is_alphanumeric': 0,\n",
              "  'is_complete_capital': 0,\n",
              "  'is_first_capital': 0,\n",
              "  'is_first_word': 0,\n",
              "  'is_last_word': 0,\n",
              "  'is_numeric': 0,\n",
              "  'next_word': 'one',\n",
              "  'prefix_1': 'n',\n",
              "  'prefix_2': 'no',\n",
              "  'prefix_3': 'not',\n",
              "  'prefix_4': 'noti',\n",
              "  'prev_word': '*-2',\n",
              "  'suffix_1': 'g',\n",
              "  'suffix_2': 'ng',\n",
              "  'suffix_3': 'ing',\n",
              "  'suffix_4': 'cing',\n",
              "  'word_has_hyphen': 0},\n",
              " {'is_alphanumeric': 0,\n",
              "  'is_complete_capital': 0,\n",
              "  'is_first_capital': 0,\n",
              "  'is_first_word': 0,\n",
              "  'is_last_word': 0,\n",
              "  'is_numeric': 0,\n",
              "  'next_word': 'another',\n",
              "  'prefix_1': 'o',\n",
              "  'prefix_2': 'on',\n",
              "  'prefix_3': 'one',\n",
              "  'prefix_4': 'one',\n",
              "  'prev_word': 'noticing',\n",
              "  'suffix_1': 'e',\n",
              "  'suffix_2': 'ne',\n",
              "  'suffix_3': 'one',\n",
              "  'suffix_4': 'one',\n",
              "  'word_has_hyphen': 0},\n",
              " {'is_alphanumeric': 0,\n",
              "  'is_complete_capital': 0,\n",
              "  'is_first_capital': 0,\n",
              "  'is_first_word': 0,\n",
              "  'is_last_word': 0,\n",
              "  'is_numeric': 0,\n",
              "  'next_word': 'only',\n",
              "  'prefix_1': 'a',\n",
              "  'prefix_2': 'an',\n",
              "  'prefix_3': 'ano',\n",
              "  'prefix_4': 'anot',\n",
              "  'prev_word': 'one',\n",
              "  'suffix_1': 'r',\n",
              "  'suffix_2': 'er',\n",
              "  'suffix_3': 'her',\n",
              "  'suffix_4': 'ther',\n",
              "  'word_has_hyphen': 0},\n",
              " {'is_alphanumeric': 0,\n",
              "  'is_complete_capital': 0,\n",
              "  'is_first_capital': 0,\n",
              "  'is_first_word': 0,\n",
              "  'is_last_word': 0,\n",
              "  'is_numeric': 0,\n",
              "  'next_word': 'when',\n",
              "  'prefix_1': 'o',\n",
              "  'prefix_2': 'on',\n",
              "  'prefix_3': 'onl',\n",
              "  'prefix_4': 'only',\n",
              "  'prev_word': 'another',\n",
              "  'suffix_1': 'y',\n",
              "  'suffix_2': 'ly',\n",
              "  'suffix_3': 'nly',\n",
              "  'suffix_4': 'only',\n",
              "  'word_has_hyphen': 0},\n",
              " {'is_alphanumeric': 0,\n",
              "  'is_complete_capital': 0,\n",
              "  'is_first_capital': 0,\n",
              "  'is_first_word': 0,\n",
              "  'is_last_word': 0,\n",
              "  'is_numeric': 0,\n",
              "  'next_word': 'they',\n",
              "  'prefix_1': 'w',\n",
              "  'prefix_2': 'wh',\n",
              "  'prefix_3': 'whe',\n",
              "  'prefix_4': 'when',\n",
              "  'prev_word': 'only',\n",
              "  'suffix_1': 'n',\n",
              "  'suffix_2': 'en',\n",
              "  'suffix_3': 'hen',\n",
              "  'suffix_4': 'when',\n",
              "  'word_has_hyphen': 0},\n",
              " {'is_alphanumeric': 0,\n",
              "  'is_complete_capital': 0,\n",
              "  'is_first_capital': 0,\n",
              "  'is_first_word': 0,\n",
              "  'is_last_word': 0,\n",
              "  'is_numeric': 0,\n",
              "  'next_word': 'jostle',\n",
              "  'prefix_1': 't',\n",
              "  'prefix_2': 'th',\n",
              "  'prefix_3': 'the',\n",
              "  'prefix_4': 'they',\n",
              "  'prev_word': 'when',\n",
              "  'suffix_1': 'y',\n",
              "  'suffix_2': 'ey',\n",
              "  'suffix_3': 'hey',\n",
              "  'suffix_4': 'they',\n",
              "  'word_has_hyphen': 0},\n",
              " {'is_alphanumeric': 0,\n",
              "  'is_complete_capital': 0,\n",
              "  'is_first_capital': 0,\n",
              "  'is_first_word': 0,\n",
              "  'is_last_word': 0,\n",
              "  'is_numeric': 0,\n",
              "  'next_word': 'for',\n",
              "  'prefix_1': 'j',\n",
              "  'prefix_2': 'jo',\n",
              "  'prefix_3': 'jos',\n",
              "  'prefix_4': 'jost',\n",
              "  'prev_word': 'they',\n",
              "  'suffix_1': 'e',\n",
              "  'suffix_2': 'le',\n",
              "  'suffix_3': 'tle',\n",
              "  'suffix_4': 'stle',\n",
              "  'word_has_hyphen': 0},\n",
              " {'is_alphanumeric': 0,\n",
              "  'is_complete_capital': 0,\n",
              "  'is_first_capital': 0,\n",
              "  'is_first_word': 0,\n",
              "  'is_last_word': 0,\n",
              "  'is_numeric': 0,\n",
              "  'next_word': 'cabs',\n",
              "  'prefix_1': 'f',\n",
              "  'prefix_2': 'fo',\n",
              "  'prefix_3': 'for',\n",
              "  'prefix_4': 'for',\n",
              "  'prev_word': 'jostle',\n",
              "  'suffix_1': 'r',\n",
              "  'suffix_2': 'or',\n",
              "  'suffix_3': 'for',\n",
              "  'suffix_4': 'for',\n",
              "  'word_has_hyphen': 0},\n",
              " {'is_alphanumeric': 0,\n",
              "  'is_complete_capital': 0,\n",
              "  'is_first_capital': 0,\n",
              "  'is_first_word': 0,\n",
              "  'is_last_word': 0,\n",
              "  'is_numeric': 0,\n",
              "  'next_word': '*T*-1',\n",
              "  'prefix_1': 'c',\n",
              "  'prefix_2': 'ca',\n",
              "  'prefix_3': 'cab',\n",
              "  'prefix_4': 'cabs',\n",
              "  'prev_word': 'for',\n",
              "  'suffix_1': 's',\n",
              "  'suffix_2': 'bs',\n",
              "  'suffix_3': 'abs',\n",
              "  'suffix_4': 'cabs',\n",
              "  'word_has_hyphen': 0},\n",
              " {'is_alphanumeric': 1,\n",
              "  'is_complete_capital': 1,\n",
              "  'is_first_capital': 0,\n",
              "  'is_first_word': 0,\n",
              "  'is_last_word': 0,\n",
              "  'is_numeric': 0,\n",
              "  'next_word': '.',\n",
              "  'prefix_1': '*',\n",
              "  'prefix_2': '*T',\n",
              "  'prefix_3': '*T*',\n",
              "  'prefix_4': '*T*-',\n",
              "  'prev_word': 'cabs',\n",
              "  'suffix_1': '1',\n",
              "  'suffix_2': '-1',\n",
              "  'suffix_3': '*-1',\n",
              "  'suffix_4': 'T*-1',\n",
              "  'word_has_hyphen': 1},\n",
              " {'is_alphanumeric': 0,\n",
              "  'is_complete_capital': 1,\n",
              "  'is_first_capital': 0,\n",
              "  'is_first_word': 0,\n",
              "  'is_last_word': 1,\n",
              "  'is_numeric': 0,\n",
              "  'next_word': '',\n",
              "  'prefix_1': '.',\n",
              "  'prefix_2': '.',\n",
              "  'prefix_3': '.',\n",
              "  'prefix_4': '.',\n",
              "  'prev_word': '*T*-1',\n",
              "  'suffix_1': '.',\n",
              "  'suffix_2': '.',\n",
              "  'suffix_3': '.',\n",
              "  'suffix_4': '.',\n",
              "  'word_has_hyphen': 0}]"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lJyEFVA-CWv1",
        "outputId": "ac0e1201-075e-4c4d-ea4c-1b588cbd9abf"
      },
      "source": [
        "y_train[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['ADP',\n",
              " 'NOUN',\n",
              " 'NOUN',\n",
              " 'NOUN',\n",
              " 'CONJ',\n",
              " 'NOUN',\n",
              " 'VERB',\n",
              " 'ADP',\n",
              " 'ADJ',\n",
              " 'NOUN',\n",
              " '.',\n",
              " 'X',\n",
              " 'VERB',\n",
              " 'NUM',\n",
              " 'DET',\n",
              " 'ADV',\n",
              " 'ADV',\n",
              " 'PRON',\n",
              " 'VERB',\n",
              " 'ADP',\n",
              " 'NOUN',\n",
              " 'X',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E7vOVvSQCnpO"
      },
      "source": [
        "Let us fit a CRF model with the default Parameters¶"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hXsOG-oLCitp",
        "outputId": "fb2610c1-c5fe-449c-dfd6-423e025dab1c"
      },
      "source": [
        "crf = CRF(\n",
        "    algorithm='lbfgs',\n",
        "    c1=0.01,\n",
        "    c2=0.1,\n",
        "    max_iterations=100,\n",
        "    all_possible_transitions=True\n",
        ")\n",
        "crf.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/base.py:197: FutureWarning: From version 0.24, get_params will raise an AttributeError if a parameter cannot be retrieved as an instance attribute. Previously it would return None.\n",
            "  FutureWarning)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CRF(algorithm='lbfgs', all_possible_states=None, all_possible_transitions=True,\n",
              "    averaging=None, c=None, c1=0.01, c2=0.1, calibration_candidates=None,\n",
              "    calibration_eta=None, calibration_max_trials=None, calibration_rate=None,\n",
              "    calibration_samples=None, delta=None, epsilon=None, error_sensitive=None,\n",
              "    gamma=None, keep_tempfiles=None, linesearch=None, max_iterations=100,\n",
              "    max_linesearch=None, min_freq=None, model_filename=None, num_memories=None,\n",
              "    pa_type=None, period=None, trainer_cls=None, variance=None, verbose=False)"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R5lUj8BJCm0J"
      },
      "source": [
        "y_pred=crf.predict(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "47fZF3oSCr7x",
        "outputId": "c622b8c5-81df-4758-8b03-a5ef7a1ec517"
      },
      "source": [
        "metrics.flat_f1_score(y_test, y_pred,average='weighted',labels=crf.classes_)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9738471726864286"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-cRTZv87Cv2H",
        "outputId": "340db20b-3e22-443d-ee18-7c4bc1cac12b"
      },
      "source": [
        "y_pred_train=crf.predict(X_train)\n",
        "metrics.flat_f1_score(y_train, y_pred_train,average='weighted',labels=crf.classes_)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9963402924209424"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SCEGWQ0FC5ju",
        "outputId": "81bafb4e-4344-4cfd-a122-95f0f99f9601"
      },
      "source": [
        "print(metrics.flat_classification_report(\n",
        "    y_test, y_pred, labels=crf.classes_, digits=3\n",
        "))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         ADP      0.979     0.985     0.982      1869\n",
            "        NOUN      0.966     0.977     0.972      5606\n",
            "        CONJ      0.994     0.994     0.994       480\n",
            "        VERB      0.964     0.960     0.962      2722\n",
            "         ADJ      0.911     0.874     0.892      1274\n",
            "           .      1.000     1.000     1.000      2354\n",
            "           X      1.000     0.997     0.998      1278\n",
            "         NUM      0.991     0.993     0.992       671\n",
            "         DET      0.994     0.995     0.994      1695\n",
            "         ADV      0.927     0.909     0.918       585\n",
            "        PRON      0.998     0.998     0.998       562\n",
            "         PRT      0.979     0.982     0.980       614\n",
            "\n",
            "    accuracy                          0.974     19710\n",
            "   macro avg      0.975     0.972     0.974     19710\n",
            "weighted avg      0.974     0.974     0.974     19710\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "voORwXhwC6L7"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wCwCoaw6DAZ5"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gMq62BNcDmBN"
      },
      "source": [
        "Ref https://nlpforhackers.io/lstm-pos-tagger-keras/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "29DBK5N1DAdb",
        "outputId": "ffdd6ad9-dcf5-4d35-aac8-b6f71ecde3e9"
      },
      "source": [
        "import nltk\n",
        "nltk.download('treebank')\n",
        "tagged_sentences = nltk.corpus.treebank.tagged_sents()\n",
        " \n",
        "print(tagged_sentences[0])\n",
        "print(\"Tagged sentences: \", len(tagged_sentences))\n",
        "print(\"Tagged words:\", len(nltk.corpus.treebank.tagged_words()))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/treebank.zip.\n",
            "[('Pierre', 'NNP'), ('Vinken', 'NNP'), (',', ','), ('61', 'CD'), ('years', 'NNS'), ('old', 'JJ'), (',', ','), ('will', 'MD'), ('join', 'VB'), ('the', 'DT'), ('board', 'NN'), ('as', 'IN'), ('a', 'DT'), ('nonexecutive', 'JJ'), ('director', 'NN'), ('Nov.', 'NNP'), ('29', 'CD'), ('.', '.')]\n",
            "Tagged sentences:  3914\n",
            "Tagged words: 100676\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_lxfWzBCDhfZ",
        "outputId": "a11408fb-aa40-4a23-9a9f-0fd9b771d9cb"
      },
      "source": [
        "import numpy as np\n",
        " \n",
        "sentences, sentence_tags =[], [] \n",
        "for tagged_sentence in tagged_sentences:\n",
        "    sentence, tags = zip(*tagged_sentence)\n",
        "    sentences.append(np.array(sentence))\n",
        "    sentence_tags.append(np.array(tags))\n",
        " \n",
        "# Let's see how a sequence looks\n",
        " \n",
        "print(sentences[5])\n",
        "print(sentence_tags[5])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Lorillard' 'Inc.' ',' 'the' 'unit' 'of' 'New' 'York-based' 'Loews'\n",
            " 'Corp.' 'that' '*T*-2' 'makes' 'Kent' 'cigarettes' ',' 'stopped' 'using'\n",
            " 'crocidolite' 'in' 'its' 'Micronite' 'cigarette' 'filters' 'in' '1956'\n",
            " '.']\n",
            "['NNP' 'NNP' ',' 'DT' 'NN' 'IN' 'JJ' 'JJ' 'NNP' 'NNP' 'WDT' '-NONE-' 'VBZ'\n",
            " 'NNP' 'NNS' ',' 'VBD' 'VBG' 'NN' 'IN' 'PRP$' 'NN' 'NN' 'NNS' 'IN' 'CD'\n",
            " '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZaZcPvEEDoSR"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        " \n",
        " \n",
        "(train_sentences, \n",
        " test_sentences, \n",
        " train_tags, \n",
        " test_tags) = train_test_split(sentences, sentence_tags, test_size=0.2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m819DhXOD1YU"
      },
      "source": [
        "**Keras also needs to work with numbers,**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MvsFPYOJDr7G"
      },
      "source": [
        "words, tags = set([]), set([])\n",
        " \n",
        "for s in train_sentences:\n",
        "    for w in s:\n",
        "        words.add(w.lower())\n",
        " \n",
        "for ts in train_tags:\n",
        "    for t in ts:\n",
        "        tags.add(t)\n",
        " \n",
        "word2index = {w: i + 2 for i, w in enumerate(list(words))}\n",
        "word2index['-PAD-'] = 0  # The special value used for padding\n",
        "word2index['-OOV-'] = 1  # The special value used for OOVs\n",
        " \n",
        "tag2index = {t: i + 1 for i, t in enumerate(list(tags))}\n",
        "tag2index['-PAD-'] = 0  # The special value used to padding"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ks9JGIPvD-a_"
      },
      "source": [
        "Let’s now convert the word dataset to integer dataset, both the words and the tags."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wzyPQOMkD7Br",
        "outputId": "f22f8248-ead9-4e9b-d2e0-8133f9c70678"
      },
      "source": [
        "train_sentences_X, test_sentences_X, train_tags_y, test_tags_y = [], [], [], []\n",
        " \n",
        "for s in train_sentences:\n",
        "    s_int = []\n",
        "    for w in s:\n",
        "        try:\n",
        "            s_int.append(word2index[w.lower()])\n",
        "        except KeyError:\n",
        "            s_int.append(word2index['-OOV-'])\n",
        " \n",
        "    train_sentences_X.append(s_int)\n",
        " \n",
        "for s in test_sentences:\n",
        "    s_int = []\n",
        "    for w in s:\n",
        "        try:\n",
        "            s_int.append(word2index[w.lower()])\n",
        "        except KeyError:\n",
        "            s_int.append(word2index['-OOV-'])\n",
        " \n",
        "    test_sentences_X.append(s_int)\n",
        " \n",
        "for s in train_tags:\n",
        "    train_tags_y.append([tag2index[t] for t in s])\n",
        " \n",
        "for s in test_tags:\n",
        "    test_tags_y.append([tag2index[t] for t in s])\n",
        " \n",
        "print(train_sentences_X[0])\n",
        "print(test_sentences_X[0])\n",
        "print(train_tags_y[0])\n",
        "print(test_tags_y[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[7679, 767, 7174, 3919, 575, 5272, 7679, 1743, 3408, 7174, 1225, 575, 3401, 9923, 787, 1225, 787, 9483, 4521, 5137]\n",
            "[9267, 1534, 255, 9058, 9923, 8879, 3104, 1025, 1622, 3994, 5404, 3104, 425, 4521, 5137]\n",
            "[29, 38, 42, 44, 38, 33, 29, 12, 12, 42, 19, 2, 22, 39, 22, 19, 22, 33, 41, 20]\n",
            "[41, 39, 21, 41, 39, 21, 26, 1, 40, 12, 21, 26, 1, 41, 20]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kLc_ECe6Dv-T",
        "outputId": "a271527d-dd84-478a-e9e3-972672f04a31"
      },
      "source": [
        "MAX_LENGTH = len(max(train_sentences_X, key=len))\n",
        "print(MAX_LENGTH)  # 271"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "128\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "smr66H4qEGCl",
        "outputId": "196e593c-5e12-4417-f8f4-f4a6760aee41"
      },
      "source": [
        "from keras.preprocessing.sequence import pad_sequences\n",
        " \n",
        "train_sentences_X = pad_sequences(train_sentences_X, maxlen=MAX_LENGTH, padding='post')\n",
        "test_sentences_X = pad_sequences(test_sentences_X, maxlen=MAX_LENGTH, padding='post')\n",
        "train_tags_y = pad_sequences(train_tags_y, maxlen=MAX_LENGTH, padding='post')\n",
        "test_tags_y = pad_sequences(test_tags_y, maxlen=MAX_LENGTH, padding='post')\n",
        " \n",
        "print(train_sentences_X[0])\n",
        "print(test_sentences_X[0])\n",
        "print(train_tags_y[0])\n",
        "print(test_tags_y[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[7679  767 7174 3919  575 5272 7679 1743 3408 7174 1225  575 3401 9923\n",
            "  787 1225  787 9483 4521 5137    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0]\n",
            "[9267 1534  255 9058 9923 8879 3104 1025 1622 3994 5404 3104  425 4521\n",
            " 5137    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0]\n",
            "[29 38 42 44 38 33 29 12 12 42 19  2 22 39 22 19 22 33 41 20  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0]\n",
            "[41 39 21 41 39 21 26  1 40 12 21 26  1 41 20  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XRFxSXgAEMkv",
        "outputId": "23493d74-d3f0-4ca9-ffd2-dd97d43128ac"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, LSTM, InputLayer, Bidirectional, TimeDistributed, Embedding, Activation\n",
        "from keras.optimizers import adam_v2\n",
        " \n",
        " \n",
        "model = Sequential()\n",
        "model.add(InputLayer(input_shape=(MAX_LENGTH, )))\n",
        "model.add(Embedding(len(word2index), 128))\n",
        "model.add(Bidirectional(LSTM(256, return_sequences=True)))\n",
        "model.add(TimeDistributed(Dense(len(tag2index))))\n",
        "model.add(Activation('softmax'))\n",
        " \n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=adam_v2.Adam(0.001),\n",
        "              metrics=['accuracy'])\n",
        " \n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 128, 128)          1289856   \n",
            "_________________________________________________________________\n",
            "bidirectional (Bidirectional (None, 128, 512)          788480    \n",
            "_________________________________________________________________\n",
            "time_distributed (TimeDistri (None, 128, 47)           24111     \n",
            "_________________________________________________________________\n",
            "activation (Activation)      (None, 128, 47)           0         \n",
            "=================================================================\n",
            "Total params: 2,102,447\n",
            "Trainable params: 2,102,447\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nIajUi2iFZl0"
      },
      "source": [
        "**One-Hot Encoded tags.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Exm5eSY4ENIG"
      },
      "source": [
        "def to_categorical(sequences, categories):\n",
        "    cat_sequences = []\n",
        "    for s in sequences:\n",
        "        cats = []\n",
        "        for item in s:\n",
        "            cats.append(np.zeros(categories))\n",
        "            cats[-1][item] = 1.0\n",
        "        cat_sequences.append(cats)\n",
        "    return np.array(cat_sequences)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qKET3zjwFbvx",
        "outputId": "c016b78f-2af7-4650-b58b-99e7f96be158"
      },
      "source": [
        "cat_train_tags_y = to_categorical(train_tags_y, len(tag2index))\n",
        "print(cat_train_tags_y[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [1. 0. 0. ... 0. 0. 0.]\n",
            " [1. 0. 0. ... 0. 0. 0.]\n",
            " [1. 0. 0. ... 0. 0. 0.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "20wG67waFdyS",
        "outputId": "0a2ada2f-c04d-417b-ebc3-bb372a7bffae"
      },
      "source": [
        "model.fit(train_sentences_X, to_categorical(train_tags_y, len(tag2index)), batch_size=128, epochs=40, validation_split=0.2)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/40\n",
            "20/20 [==============================] - 12s 174ms/step - loss: 1.6232 - accuracy: 0.7595 - val_loss: 0.7547 - val_accuracy: 0.8025\n",
            "Epoch 2/40\n",
            "20/20 [==============================] - 2s 119ms/step - loss: 0.6986 - accuracy: 0.8046 - val_loss: 0.6683 - val_accuracy: 0.8114\n",
            "Epoch 3/40\n",
            "20/20 [==============================] - 2s 119ms/step - loss: 0.6485 - accuracy: 0.8182 - val_loss: 0.6345 - val_accuracy: 0.8271\n",
            "Epoch 4/40\n",
            "20/20 [==============================] - 2s 119ms/step - loss: 0.6199 - accuracy: 0.8250 - val_loss: 0.6065 - val_accuracy: 0.8254\n",
            "Epoch 5/40\n",
            "20/20 [==============================] - 2s 118ms/step - loss: 0.5972 - accuracy: 0.8265 - val_loss: 0.5901 - val_accuracy: 0.8311\n",
            "Epoch 6/40\n",
            "20/20 [==============================] - 2s 117ms/step - loss: 0.5885 - accuracy: 0.8284 - val_loss: 0.5858 - val_accuracy: 0.8412\n",
            "Epoch 7/40\n",
            "20/20 [==============================] - 2s 118ms/step - loss: 0.5774 - accuracy: 0.8358 - val_loss: 0.5701 - val_accuracy: 0.8407\n",
            "Epoch 8/40\n",
            "20/20 [==============================] - 2s 118ms/step - loss: 0.5659 - accuracy: 0.8426 - val_loss: 0.5577 - val_accuracy: 0.8502\n",
            "Epoch 9/40\n",
            "20/20 [==============================] - 2s 118ms/step - loss: 0.5490 - accuracy: 0.8556 - val_loss: 0.5388 - val_accuracy: 0.8701\n",
            "Epoch 10/40\n",
            "20/20 [==============================] - 2s 118ms/step - loss: 0.5238 - accuracy: 0.8742 - val_loss: 0.5094 - val_accuracy: 0.8770\n",
            "Epoch 11/40\n",
            "20/20 [==============================] - 2s 118ms/step - loss: 0.4858 - accuracy: 0.8836 - val_loss: 0.4667 - val_accuracy: 0.8897\n",
            "Epoch 12/40\n",
            "20/20 [==============================] - 2s 118ms/step - loss: 0.4353 - accuracy: 0.8955 - val_loss: 0.4162 - val_accuracy: 0.8962\n",
            "Epoch 13/40\n",
            "20/20 [==============================] - 2s 118ms/step - loss: 0.3801 - accuracy: 0.9049 - val_loss: 0.3629 - val_accuracy: 0.9050\n",
            "Epoch 14/40\n",
            "20/20 [==============================] - 2s 117ms/step - loss: 0.3253 - accuracy: 0.9148 - val_loss: 0.3138 - val_accuracy: 0.9141\n",
            "Epoch 15/40\n",
            "20/20 [==============================] - 2s 118ms/step - loss: 0.2748 - accuracy: 0.9271 - val_loss: 0.2687 - val_accuracy: 0.9279\n",
            "Epoch 16/40\n",
            "20/20 [==============================] - 2s 118ms/step - loss: 0.2273 - accuracy: 0.9428 - val_loss: 0.2263 - val_accuracy: 0.9434\n",
            "Epoch 17/40\n",
            "20/20 [==============================] - 2s 118ms/step - loss: 0.1801 - accuracy: 0.9595 - val_loss: 0.1852 - val_accuracy: 0.9562\n",
            "Epoch 18/40\n",
            "20/20 [==============================] - 2s 119ms/step - loss: 0.1373 - accuracy: 0.9721 - val_loss: 0.1514 - val_accuracy: 0.9645\n",
            "Epoch 19/40\n",
            "20/20 [==============================] - 2s 117ms/step - loss: 0.1035 - accuracy: 0.9799 - val_loss: 0.1268 - val_accuracy: 0.9702\n",
            "Epoch 20/40\n",
            "20/20 [==============================] - 2s 118ms/step - loss: 0.0788 - accuracy: 0.9850 - val_loss: 0.1090 - val_accuracy: 0.9737\n",
            "Epoch 21/40\n",
            "20/20 [==============================] - 2s 117ms/step - loss: 0.0624 - accuracy: 0.9882 - val_loss: 0.0980 - val_accuracy: 0.9762\n",
            "Epoch 22/40\n",
            "20/20 [==============================] - 2s 118ms/step - loss: 0.0508 - accuracy: 0.9906 - val_loss: 0.0910 - val_accuracy: 0.9772\n",
            "Epoch 23/40\n",
            "20/20 [==============================] - 2s 118ms/step - loss: 0.0427 - accuracy: 0.9920 - val_loss: 0.0853 - val_accuracy: 0.9786\n",
            "Epoch 24/40\n",
            "20/20 [==============================] - 2s 117ms/step - loss: 0.0363 - accuracy: 0.9932 - val_loss: 0.0818 - val_accuracy: 0.9791\n",
            "Epoch 25/40\n",
            "20/20 [==============================] - 2s 118ms/step - loss: 0.0314 - accuracy: 0.9940 - val_loss: 0.0790 - val_accuracy: 0.9799\n",
            "Epoch 26/40\n",
            "20/20 [==============================] - 2s 118ms/step - loss: 0.0275 - accuracy: 0.9947 - val_loss: 0.0773 - val_accuracy: 0.9803\n",
            "Epoch 27/40\n",
            "20/20 [==============================] - 2s 117ms/step - loss: 0.0244 - accuracy: 0.9953 - val_loss: 0.0750 - val_accuracy: 0.9807\n",
            "Epoch 28/40\n",
            "20/20 [==============================] - 2s 118ms/step - loss: 0.0218 - accuracy: 0.9959 - val_loss: 0.0736 - val_accuracy: 0.9809\n",
            "Epoch 29/40\n",
            "20/20 [==============================] - 2s 117ms/step - loss: 0.0196 - accuracy: 0.9962 - val_loss: 0.0731 - val_accuracy: 0.9812\n",
            "Epoch 30/40\n",
            "20/20 [==============================] - 2s 118ms/step - loss: 0.0177 - accuracy: 0.9966 - val_loss: 0.0739 - val_accuracy: 0.9808\n",
            "Epoch 31/40\n",
            "20/20 [==============================] - 2s 119ms/step - loss: 0.0162 - accuracy: 0.9970 - val_loss: 0.0725 - val_accuracy: 0.9811\n",
            "Epoch 32/40\n",
            "20/20 [==============================] - 2s 118ms/step - loss: 0.0147 - accuracy: 0.9973 - val_loss: 0.0730 - val_accuracy: 0.9812\n",
            "Epoch 33/40\n",
            "20/20 [==============================] - 2s 119ms/step - loss: 0.0138 - accuracy: 0.9974 - val_loss: 0.0729 - val_accuracy: 0.9812\n",
            "Epoch 34/40\n",
            "20/20 [==============================] - 2s 120ms/step - loss: 0.0128 - accuracy: 0.9976 - val_loss: 0.0713 - val_accuracy: 0.9815\n",
            "Epoch 35/40\n",
            "20/20 [==============================] - 2s 118ms/step - loss: 0.0117 - accuracy: 0.9978 - val_loss: 0.0731 - val_accuracy: 0.9813\n",
            "Epoch 36/40\n",
            "20/20 [==============================] - 2s 118ms/step - loss: 0.0109 - accuracy: 0.9980 - val_loss: 0.0719 - val_accuracy: 0.9816\n",
            "Epoch 37/40\n",
            "20/20 [==============================] - 2s 118ms/step - loss: 0.0100 - accuracy: 0.9981 - val_loss: 0.0736 - val_accuracy: 0.9813\n",
            "Epoch 38/40\n",
            "20/20 [==============================] - 2s 119ms/step - loss: 0.0093 - accuracy: 0.9982 - val_loss: 0.0747 - val_accuracy: 0.9810\n",
            "Epoch 39/40\n",
            "20/20 [==============================] - 2s 119ms/step - loss: 0.0087 - accuracy: 0.9983 - val_loss: 0.0751 - val_accuracy: 0.9811\n",
            "Epoch 40/40\n",
            "20/20 [==============================] - 2s 119ms/step - loss: 0.0081 - accuracy: 0.9985 - val_loss: 0.0742 - val_accuracy: 0.9813\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f414a2ad110>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3eLA8PbRFgET",
        "outputId": "8b75ac42-90c2-404b-ff65-9fa1a07b5609"
      },
      "source": [
        "scores = model.evaluate(test_sentences_X, to_categorical(test_tags_y, len(tag2index)))\n",
        "print(f\"{model.metrics_names[1]}: {scores[1] * 100}\")   # acc: 99.09751977804825\n",
        " "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "25/25 [==============================] - 1s 34ms/step - loss: 0.0745 - accuracy: 0.9814\n",
            "accuracy: 98.14016819000244\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eIJUZT3oHKCU",
        "outputId": "ff40f621-3362-4279-860e-f2c8914f4275"
      },
      "source": [
        "test_samples = [\n",
        "    \"running is very important for me .\".split(),\n",
        "    \"I was running every day for a month .\".split()\n",
        "]\n",
        "print(test_samples)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['running', 'is', 'very', 'important', 'for', 'me', '.'], ['I', 'was', 'running', 'every', 'day', 'for', 'a', 'month', '.']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R2OAWCJiHKF_",
        "outputId": "5c3d4952-d4f1-4d98-c6e4-a2d0a13ab60a"
      },
      "source": [
        "test_samples_X = []\n",
        "for s in test_samples:\n",
        "    s_int = []\n",
        "    for w in s:\n",
        "        try:\n",
        "            s_int.append(word2index[w.lower()])\n",
        "        except KeyError:\n",
        "            s_int.append(word2index['-OOV-'])\n",
        "    test_samples_X.append(s_int)\n",
        " \n",
        "test_samples_X = pad_sequences(test_samples_X, maxlen=MAX_LENGTH, padding='post')\n",
        "print(test_samples_X)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[3124 3919  464 1941 5272 2997 5137    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0]\n",
            " [9267 4457 3124 9394 4251 5272 3952 1485 5137    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sLBPH-wcHO9v",
        "outputId": "5a90a257-64e7-48d4-857f-df238146a088"
      },
      "source": [
        "predictions = model.predict(test_samples_X)\n",
        "print(predictions, predictions.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[[2.83112720e-04 5.65613445e-04 1.15833466e-03 ... 9.13295243e-03\n",
            "   2.27518035e-06 4.32581983e-05]\n",
            "  [1.09070343e-05 1.20216409e-05 9.25535971e-07 ... 9.92994130e-01\n",
            "   2.00588275e-07 4.57721262e-06]\n",
            "  [4.33478817e-05 4.69316932e-04 1.21529889e-03 ... 1.92207191e-03\n",
            "   1.03022765e-04 3.17449599e-06]\n",
            "  ...\n",
            "  [9.99973893e-01 7.66782159e-06 7.65984343e-09 ... 3.24443805e-10\n",
            "   4.99227826e-06 1.31795090e-07]\n",
            "  [9.99958038e-01 1.39727435e-05 8.53342463e-09 ... 3.93045041e-10\n",
            "   8.69602809e-06 1.72742872e-07]\n",
            "  [9.99927640e-01 2.61714085e-05 1.06789457e-08 ... 5.08691533e-10\n",
            "   1.43297557e-05 2.41697620e-07]]\n",
            "\n",
            " [[3.27781436e-06 8.73282770e-05 3.98219745e-05 ... 2.15375656e-03\n",
            "   1.71466311e-07 7.27040879e-06]\n",
            "  [3.22034089e-06 3.25348879e-06 3.01405088e-08 ... 5.20505535e-04\n",
            "   4.57724326e-07 5.09031906e-06]\n",
            "  [1.76283920e-05 2.67665531e-03 1.95851699e-05 ... 8.73912999e-04\n",
            "   4.45054809e-07 3.72678073e-07]\n",
            "  ...\n",
            "  [9.99973893e-01 7.66020548e-06 7.66533859e-09 ... 3.24128946e-10\n",
            "   4.99837643e-06 1.31955190e-07]\n",
            "  [9.99958038e-01 1.39595586e-05 8.53923776e-09 ... 3.92680860e-10\n",
            "   8.70615168e-06 1.72943629e-07]\n",
            "  [9.99927759e-01 2.61479836e-05 1.06858948e-08 ... 5.08244502e-10\n",
            "   1.43457282e-05 2.41966774e-07]]] (2, 128, 47)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O9Agu_xNHTZF"
      },
      "source": [
        "def logits_to_tokens(sequences, index):\n",
        "    token_sequences = []\n",
        "    for categorical_sequence in sequences:\n",
        "        token_sequence = []\n",
        "        for categorical in categorical_sequence:\n",
        "            token_sequence.append(index[np.argmax(categorical)])\n",
        " \n",
        "        token_sequences.append(token_sequence)\n",
        " \n",
        "    return token_sequences"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6L5vkHCfHTcE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "93e27782-1536-4977-b316-3ddc8c82fdf8"
      },
      "source": [
        "print(logits_to_tokens(predictions, {i: t for t, i in tag2index.items()}))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['VBG', 'VBZ', 'RB', 'JJ', 'IN', 'PRP', '.', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-'], ['PRP', 'VBD', 'VBG', 'DT', 'NN', 'IN', 'DT', 'NN', '.', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D6AGX43UHXE4"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}